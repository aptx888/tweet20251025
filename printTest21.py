#âœ… å®Œæ•´å¯è¿è¡Œç‰ˆæœ¬ï¼š
#æ”¯æŒè‡ªåŠ¨å®šæ—¶æŠ“å– + è‡ªåŠ¨è¿½åŠ ä¿å­˜ + Aåˆ—ç¼–å· + UTF-8 ç¼–ç  + Windows/Excelå…¼å®¹


import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime
import time
import os


# === å¯åŠ¨ Chrome WebDriver ===
def start_driver():
    driver_path = ChromeDriverManager().install()
    service = Service(driver_path)
    driver = webdriver.Chrome(service=service)
    return driver


# === æŠ“å–æ¨æ–‡ä¸­ â€œä»¶ã®è¡¨ç¤ºâ€ å¼€å§‹çš„å†…å®¹ ===
def scrape_tweet(tweet_url):
    driver = start_driver()
    driver.get(tweet_url)
    time.sleep(5)  # ç­‰å¾…ç½‘é¡µåŠ è½½å®Œæ¯•ï¼Œå¯æ ¹æ®ç½‘é€Ÿè°ƒæ•´

    start_printing = False
    collected_text = ""

    try:
        all_elements = driver.find_elements(By.XPATH, "//div | //span")
        for element in all_elements:
            text = element.text.strip()
            if "ä»¶ã®è¡¨ç¤º" in text and not start_printing:
                start_printing = True
                start_index = text.index("ä»¶ã®è¡¨ç¤º")
                collected_text = text[start_index:start_index + 30]
            if start_printing and collected_text:
                break
    except Exception as e:
        print(f"âŒ æŠ“å–æ—¶å‡ºé”™: {e}")
    finally:
        driver.quit()

    return collected_text


# === ä¿å­˜æ•°æ®åˆ° CSV ===
def save_to_csv(data, output_file='data.csv'):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    new_row = {"Time": timestamp, "Data": data}

    if os.path.exists(output_file):
        df = pd.read_csv(output_file, encoding='utf-8-sig')
        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
    else:
        df = pd.DataFrame([new_row])

    # è‡ªåŠ¨ç¼–å·ï¼ˆç´¢å¼•ä»1å¼€å§‹ï¼‰
    df.index += 1
    df.to_csv(output_file, index=True, encoding='utf-8-sig')
    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ° {output_file}")


# === ä¸»å¾ªç¯ï¼šæ¯éš”30åˆ†é’Ÿè‡ªåŠ¨è¿è¡Œä¸€æ¬¡ ===
def auto_run(tweet_url, interval_minutes=30, output_file='data.csv'):
    print(f"ğŸš€ å¯åŠ¨è‡ªåŠ¨æŠ“å–ç¨‹åºï¼Œæ¯éš” {interval_minutes} åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ã€‚")
    print("æŒ‰ Ctrl + C å¯æ‰‹åŠ¨åœæ­¢ã€‚")

    while True:
        print("\n=== å¼€å§‹æŠ“å–æ•°æ® ===")
        data = scrape_tweet(tweet_url)
        if data:
            print(f"æŠ“å–ç»“æœ: {data}")
            save_to_csv(data, output_file)
        else:
            print("âš ï¸ æ²¡æœ‰æŠ“å–åˆ°å†…å®¹ï¼Œå¯èƒ½æ˜¯ç½‘ç»œæˆ–é¡µé¢åŠ è½½é—®é¢˜ã€‚")

        print(f"â° ç­‰å¾… {interval_minutes} åˆ†é’Ÿåå†æ¬¡è¿è¡Œ...\n")
        time.sleep(interval_minutes * 60)  # è½¬æ¢ä¸ºç§’


# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    tweet_url = "https://x.com/dramaDIVE_ytv/status/1981567801405161653"  # â† æ¢æˆçœŸå®æ¨æ–‡é“¾æ¥
    auto_run(tweet_url, interval_minutes=30)
